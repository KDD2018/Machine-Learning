{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)\n",
      "tensorflow 2.0.0-beta1\n",
      "numpy 1.18.1\n",
      "pandas 0.24.1\n",
      "sklearn 0.21.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in tf, np, pd, sklearn:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = './data/csv/'\n",
    "\n",
    "def get_filenames_by_prefix(source_dir, prefix_name):\n",
    "    all_files = os.listdir(source_dir)\n",
    "    results = []\n",
    "    for filename in all_files:\n",
    "        if filename.startswith(prefix_name):\n",
    "            results.append(os.path.join(source_dir, filename))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = get_filenames_by_prefix(source_dir, 'train')\n",
    "test_filenames = get_filenames_by_prefix(source_dir, 'test')\n",
    "valid_filenames = get_filenames_by_prefix(source_dir, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/csv/train_14.csv',\n",
      " './data/csv/train_18.csv',\n",
      " './data/csv/train_ 7.csv',\n",
      " './data/csv/train_10.csv',\n",
      " './data/csv/train_ 9.csv',\n",
      " './data/csv/train_ 5.csv',\n",
      " './data/csv/train_ 8.csv',\n",
      " './data/csv/train_12.csv',\n",
      " './data/csv/train_19.csv',\n",
      " './data/csv/train_11.csv',\n",
      " './data/csv/train_ 1.csv',\n",
      " './data/csv/train_15.csv',\n",
      " './data/csv/train_ 3.csv',\n",
      " './data/csv/train_ 2.csv',\n",
      " './data/csv/train_ 0.csv',\n",
      " './data/csv/train_13.csv',\n",
      " './data/csv/train_ 4.csv',\n",
      " './data/csv/train_16.csv',\n",
      " './data/csv/train_17.csv',\n",
      " './data/csv/train_ 6.csv']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个解析csv每行内容的方法\n",
    "def parse_csv_line(line, n_fields=9):\n",
    "    records = [tf.constant(np.nan)]*n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, records)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1])\n",
    "    return x, y\n",
    "\n",
    "# 定义一个多线程读取csv文件并解析的方法\n",
    "def csv_read_dataset(filenames, n_readers=5, batch_size=32, n_parse_threads=5, shuffle_buffer_size=10000):\n",
    "    '''\n",
    "    filenames: 文件名列表\n",
    "    n_readers: 并行程度\n",
    "    batch_size: 批大小\n",
    "    n_parse_threads: 解析的并行程度\n",
    "    shuffle_buffer_size: \n",
    "    '''\n",
    "    filenames_dataset = tf.data.Dataset.list_files(filenames)\n",
    "    filenames_dataset = filenames_dataset.repeat()\n",
    "    dataset = filenames_dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length=n_readers)\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_read_dataset(train_filenames, batch_size=32)\n",
    "valid_set = csv_read_dataset(valid_filenames, batch_size=32)\n",
    "test_set = csv_read_dataset(test_filenames, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(x, y):\n",
    "    '''Convert x,y to example and serialize'''\n",
    "    input_features = tf.train.FloatList(value=x)\n",
    "    label = tf.train.FloatList(value=[y])\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            'input_features': tf.train.Feature(float_list=input_features),\n",
    "            'label': tf.train.Feature(float_list=label)\n",
    "        }\n",
    "    )\n",
    "    example = tf.train.Example(features=features)\n",
    "    return example.SerializeToString()\n",
    "\n",
    "def csv_dataset_to_tfrecord(filename, dataset, n_shards, steps_per_shard, compression_type=None):\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    all_filenames = []\n",
    "    for shard_id in range(n_shards):\n",
    "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(filename, shard_id, n_shards)\n",
    "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
    "            for x_batch, y_batch in dataset.take(steps_per_shard):\n",
    "                for x, y in zip(x_batch, y_batch):\n",
    "                    writer.write(serialize_example(x, y))\n",
    "        all_filenames.append(filename_fullpath)\n",
    "    return all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shards = 10\n",
    "batch_size = 32\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880 // batch_size // n_shards\n",
    "test_steps_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "output_dir = './data/tfrecord'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/tfrecord/train\n"
     ]
    }
   ],
   "source": [
    "train_basename = os.path.join(output_dir, 'train')\n",
    "test_basename = os.path.join(output_dir, 'test')\n",
    "valid_basename = os.path.join(output_dir, 'valid')\n",
    "\n",
    "print(train_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tfrecord_filenames = csv_dataset_to_tfrecord(\n",
    "    train_basename, train_set, n_shards, train_steps_per_shard, None)\n",
    "test_tfrecord_filenames = csv_dataset_to_tfrecord(\n",
    "    test_basename, test_set, n_shards, test_steps_per_shard, None)\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecord(\n",
    "    valid_basename, valid_set, n_shards, valid_steps_per_shard, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/tfrecord_zip/train\n"
     ]
    }
   ],
   "source": [
    "# 生成压缩文件\n",
    "\n",
    "output_dir = './data/tfrecord_zip'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir, 'train')\n",
    "test_basename = os.path.join(output_dir, 'test')\n",
    "valid_basename = os.path.join(output_dir, 'valid')\n",
    "\n",
    "print(train_basename)\n",
    "\n",
    "train_tfrecordZIP_filenames = csv_dataset_to_tfrecord(\n",
    "    train_basename, train_set, n_shards, train_steps_per_shard, compression_type='GZIP')\n",
    "test_tfrecordZIP_filenames = csv_dataset_to_tfrecord(\n",
    "    test_basename, test_set, n_shards, test_steps_per_shard, compression_type='GZIP')\n",
    "valid_tfrecordZIP_filenames = csv_dataset_to_tfrecord(\n",
    "    valid_basename, valid_set, n_shards, valid_steps_per_shard, compression_type='GZIP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/tfrecord_zip/train_00000-of-00010',\n",
      " './data/tfrecord_zip/train_00001-of-00010',\n",
      " './data/tfrecord_zip/train_00002-of-00010',\n",
      " './data/tfrecord_zip/train_00003-of-00010',\n",
      " './data/tfrecord_zip/train_00004-of-00010',\n",
      " './data/tfrecord_zip/train_00005-of-00010',\n",
      " './data/tfrecord_zip/train_00006-of-00010',\n",
      " './data/tfrecord_zip/train_00007-of-00010',\n",
      " './data/tfrecord_zip/train_00008-of-00010',\n",
      " './data/tfrecord_zip/train_00009-of-00010']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_tfrecordZIP_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_features = {\n",
    "    'input_features': tf.io.FixedLenFeature(shape=[8], dtype=tf.float32),\n",
    "    'label': tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "def parse(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example, expected_features)\n",
    "    return example['input_features'], example['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecords_reader_dataset(filenames, n_readers=5, batch_size=32, n_parse_threads=5, shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TFRecordDataset(filename, compression_type='GZIP'),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "tfrecords_train_set = tfrecords_reader_dataset(train_tfrecordZIP_filenames, batch_size=32)\n",
    "tfrecords_test_set = tfrecords_reader_dataset(test_tfrecordZIP_filenames, batch_size=32)\n",
    "tfrecords_valid_set = tfrecords_reader_dataset(valid_tfrecordZIP_filenames, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
      "  -0.69240725  0.72652334]\n",
      " [ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
      "  -0.69240725  0.72652334]\n",
      " [ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
      "  -0.69240725  0.72652334]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.419]\n",
      " [2.419]\n",
      " [2.419]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
      "  -0.69240725  0.72652334]\n",
      " [ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
      "  -0.69240725  0.72652334]\n",
      " [ 0.04326301 -1.0895426  -0.38878718 -0.10789865 -0.68186635 -0.0723871\n",
      "  -0.8883662   0.8213992 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.419]\n",
      " [2.419]\n",
      " [1.426]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_ = tfrecords_reader_dataset(train_tfrecordZIP_filenames, batch_size=3)\n",
    "for x_batch, y_batch in train_.take(2):\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.9549 - val_loss: 0.6289\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.5649 - val_loss: 0.5633\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.5099 - val_loss: 0.5300\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4614 - val_loss: 0.4753\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4647 - val_loss: 0.4871\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.4381 - val_loss: 0.4408\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4317 - val_loss: 0.4681\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4230 - val_loss: 0.4394\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4192 - val_loss: 0.4182\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4118 - val_loss: 0.4276\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.4060 - val_loss: 0.4180\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3997 - val_loss: 0.4001\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3973 - val_loss: 0.4006\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3935 - val_loss: 0.3987\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3854 - val_loss: 0.4007\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3823 - val_loss: 0.3968\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3774 - val_loss: 0.4002\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3806 - val_loss: 0.4061\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3651 - val_loss: 0.3746\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3772 - val_loss: 0.4279\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3669 - val_loss: 0.3710\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3672 - val_loss: 0.4126\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3639 - val_loss: 0.3909\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3638 - val_loss: 0.3617\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3610 - val_loss: 0.3739\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3611 - val_loss: 0.3805\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3568 - val_loss: 0.3693\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3564 - val_loss: 0.3660\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3537 - val_loss: 0.3723\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation='relu', input_shape=[8]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3)]\n",
    "history = model.fit(tfrecords_train_set,\n",
    "                   validation_data = tfrecords_valid_set,\n",
    "                   steps_per_epoch=11160 // batch_size,\n",
    "                   validation_steps=3870 // batch_size,\n",
    "                   epochs=100,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.3868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3868438932836426"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tfrecords_test_set, steps=5160//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
