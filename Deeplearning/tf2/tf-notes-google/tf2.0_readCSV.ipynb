{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc3\n",
      "sys.version_info(major=3, minor=8, micro=2, releaselevel='final', serial=0)\n",
      "tensorflow 2.2.0-rc3\n",
      "numpy 1.18.3\n",
      "pandas 1.0.3\n",
      "sklearn 0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in tf, np, pd, sklearn:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从numpy数组构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# repeat 将数据重复几次，相当于epoch的作用\n",
    "# batch 每批次获取的样本数量\n",
    "dataset1 = dataset.repeat(4).batch(5)\n",
    "for item in dataset1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.repeat(3).batch(7):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# interleave 将dataset中元素进行处理，相当于map的功能\n",
    "# cycle_length 多少个Tensor一个循环\n",
    "# block_lenght 一次获取v的多少个元素\n",
    "dataset2 = dataset1.interleave(lambda v: tf.data.Dataset.from_tensor_slices(v), cycle_length=4, block_length=3)\n",
    "for item in dataset2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从元组/字典构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3])>, <tf.Tensor: shape=(), dtype=string, numpy=b'Beijing'>)\n",
      "(<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 3, 4])>, <tf.Tensor: shape=(), dtype=string, numpy=b'Shanghai'>)\n",
      "(<tf.Tensor: shape=(3,), dtype=int64, numpy=array([3, 4, 5])>, <tf.Tensor: shape=(), dtype=string, numpy=b'Chongqing'>)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3], [2,3,4], [3,4,5]])\n",
    "y = np.array(['Beijing', 'Shanghai', 'Chongqing'])\n",
    "dataset3 = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "for item in dataset3:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] Beijing\n",
      "[2 3 4] Shanghai\n",
      "[3 4 5] Chongqing\n"
     ]
    }
   ],
   "source": [
    "for item_x, item_y in dataset3:\n",
    "    print(item_x.numpy(), item_y.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int64) tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
      "tf.Tensor([2 3 4], shape=(3,), dtype=int64) tf.Tensor(b'Shanghai', shape=(), dtype=string)\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int64) tf.Tensor(b'Chongqing', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset3:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coordinate': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3])>, 'city': <tf.Tensor: shape=(), dtype=string, numpy=b'Beijing'>}\n",
      "{'coordinate': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 3, 4])>, 'city': <tf.Tensor: shape=(), dtype=string, numpy=b'Shanghai'>}\n",
      "{'coordinate': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([3, 4, 5])>, 'city': <tf.Tensor: shape=(), dtype=string, numpy=b'Chongqing'>}\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices({'coordinate': x, 'city': y})\n",
    "for item in ds:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Beijing' [1 2 3]\n",
      "b'Shanghai' [2 3 4]\n",
      "b'Chongqing' [3 4 5]\n"
     ]
    }
   ],
   "source": [
    "for item in tf.data.Dataset.from_tensor_slices({'coordinate': x, 'city': y}):\n",
    "    print(item['city'].numpy(), item['coordinate'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['coordinate', 'city'])\n",
      "dict_keys(['coordinate', 'city'])\n",
      "dict_keys(['coordinate', 'city'])\n"
     ]
    }
   ],
   "source": [
    "for item in ds:\n",
    "    print(item.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
      "tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n",
      "tf.Tensor(b'Shanghai', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n",
      "tf.Tensor(b'Chongqing', shape=(), dtype=string)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in ds:\n",
    "    print(item['coordinate'])\n",
    "    print(item['city'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/kdd/scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "# 获取california_housing数据集\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "house = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15480, 8) (15480,)\n",
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(house.data, house.target,random_state=7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=11)\n",
    "print(x_train_all.shape, y_train_all.shape)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./data/csv/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# 将feature和label拼接起来   \n",
    "train_data = np.c_[x_train_scaled, y_train]\n",
    "valid_data = np.c_[x_valid_scaled, y_valid]\n",
    "test_data = np.c_[x_test_scaled, y_test]\n",
    "# 获取字段名称\n",
    "house_cols = house.feature_names + ['MHV']\n",
    "cols_str = ','.join(house_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个保存csv文件的方法， 也可以转化为pd.DataFrame然后写入csv\n",
    "def save_to_csv(output_dir, data, name_prefix, header=None, n_parts=10):\n",
    "    '''\n",
    "    output_dir: 保存路径\n",
    "    data: 要保存的数据\n",
    "    name_prefix: 保存的文件名前缀\n",
    "    header: 表头\n",
    "    n_parts: 保存几部分\n",
    "    '''\n",
    "    path_format = os.path.join(output_dir, '{}_{:2d}.csv')\n",
    "    filenames = []\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(len(data)), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv, 'wt', encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header+'\\n')\n",
    "            for row_index in row_indices:\n",
    "                f.write(','.join([repr(col) for col in data[row_index]]))\n",
    "                f.write('\\n')\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = save_to_csv(output_dir, train_data, 'train', cols_str, n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, 'valid', cols_str, n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir, test_data, 'test', cols_str, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_filenames:\n",
      "['./data/csv/train_ 0.csv',\n",
      " './data/csv/train_ 1.csv',\n",
      " './data/csv/train_ 2.csv',\n",
      " './data/csv/train_ 3.csv',\n",
      " './data/csv/train_ 4.csv',\n",
      " './data/csv/train_ 5.csv',\n",
      " './data/csv/train_ 6.csv',\n",
      " './data/csv/train_ 7.csv',\n",
      " './data/csv/train_ 8.csv',\n",
      " './data/csv/train_ 9.csv',\n",
      " './data/csv/train_10.csv',\n",
      " './data/csv/train_11.csv',\n",
      " './data/csv/train_12.csv',\n",
      " './data/csv/train_13.csv',\n",
      " './data/csv/train_14.csv',\n",
      " './data/csv/train_15.csv',\n",
      " './data/csv/train_16.csv',\n",
      " './data/csv/train_17.csv',\n",
      " './data/csv/train_18.csv',\n",
      " './data/csv/train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print('train_filenames:')\n",
    "pprint.pprint(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/csv/train_ 0.csv', './data/csv/train_ 1.csv', './data/csv/train_ 2.csv', './data/csv/train_ 3.csv', './data/csv/train_ 4.csv', './data/csv/train_ 5.csv', './data/csv/train_ 6.csv', './data/csv/train_ 7.csv', './data/csv/train_ 8.csv', './data/csv/train_ 9.csv', './data/csv/train_10.csv', './data/csv/train_11.csv', './data/csv/train_12.csv', './data/csv/train_13.csv', './data/csv/train_14.csv', './data/csv/train_15.csv', './data/csv/train_16.csv', './data/csv/train_17.csv', './data/csv/train_18.csv', './data/csv/train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "print(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset shapes: (), types: tf.string>\n",
      "tf.Tensor(b'./data/csv/train_ 4.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 8.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 5.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 6.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 3.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 0.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 2.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 1.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 9.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 7.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 构建文件名数据集\n",
    "filenames_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "print(filenames_dataset)\n",
    "for filename in filenames_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6363646332204844,-1.0895425985107923,0.09260902815633619,-0.20538124656801682,1.2025670451003232,-0.03630122549633783,-0.6784101660505877,0.182235342347858,2.429\n",
      "0.42408210084996534,0.9129633171802288,-0.04437481876046234,-0.15297213746739335,-0.24727627804141977,-0.10539166599677323,0.8612674255663844,-1.3357789003702432,3.955\n",
      "2.51504373119231,1.0731637904355105,0.5574401201546321,-0.17273513019187772,-0.612912610473286,-0.01909156503651574,-0.5710993036045546,-0.027490309606616956,5.00001\n",
      "0.04326300977263167,-1.0895425985107923,-0.38878716774583305,-0.10789864528874438,-0.6818663605100649,-0.0723871014747467,-0.8883662012710817,0.8213992340186296,1.426\n",
      "-0.32652634129448693,0.43236189741438374,-0.09345459539684739,-0.08402991822890092,0.8460035745154013,-0.0266316482653991,-0.5617679242614233,0.1422875991184281,2.431\n"
     ]
    }
   ],
   "source": [
    "n_readers = 5\n",
    "dataset = filenames_dataset.interleave(lambda filename: tf.data.TextLineDataset(filename).skip(1), cycle_length=n_readers)\n",
    "for line in dataset.take(5): # 取前5行\n",
    "    print(line.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor(b'4', shape=(), dtype=string)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 解析csv中的内容， 举个栗子\n",
    "sample_str = '1,2,3,4,5'\n",
    "records = [\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    0,\n",
    "    np.nan,\n",
    "    'hell',\n",
    "    tf.constant([])\n",
    "]\n",
    "parsed_fields = tf.io.decode_csv(sample_str, records)\n",
    "for item in parsed_fields:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Field 4 is required but missing in record 0! [Op:DecodeCSV]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DecodeCSV\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-18d909f1d3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',,,,'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv_v2\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[1;32m   1018\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mselect_cols\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of select_cols and record_defaults do not match.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m   return gen_parsing_ops.decode_csv(\n\u001b[0m\u001b[1;32m   1021\u001b[0m       \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m       \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord_defaults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         return decode_csv_eager_fallback(\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfield_delim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0muse_quote_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_quote_delim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv_eager_fallback\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name, ctx)\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;34m\"use_quote_delim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_quote_delim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"na_value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"select_cols\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   select_cols)\n\u001b[0;32m--> 131\u001b[0;31m   _result = _execute.execute(b\"DecodeCSV\", len(record_defaults),\n\u001b[0m\u001b[1;32m    132\u001b[0m                              \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_inputs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                              name=name)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Field 4 is required but missing in record 0! [Op:DecodeCSV]"
     ]
    }
   ],
   "source": [
    "tf.io.decode_csv(',,,,', records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.io.decode_csv(',,,,', records)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 fields but have 3 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.io.decode_csv('11,2,5', records)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个解析csv每行内容的方法\n",
    "def parse_csv_line(line, n_fields):\n",
    "    records = [tf.constant(np.nan)]*n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, records)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.63034356,  1.8741661 , -0.06713215, -0.12543367, -0.19737554,\n",
       "        -0.02272263, -0.69240725,  0.72652334], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.419>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_csv_line(b'0.6303435674178064,1.874166156711919,-0.06713214279531016,-0.12543366804152128,-0.19737553788322462,-0.022722631725889016,-0.692407235065288,0.7265233438487496,2.419',n_fields=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个解析csv每行内容的方法\n",
    "def parse_csv_line(line, n_fields=9):\n",
    "    records = [tf.constant(np.nan)]*n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, records)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1])\n",
    "    return x, y\n",
    "\n",
    "# 定义一个多线程读取csv文件并解析的方法\n",
    "def csv_read_dataset(filenames, n_readers=5, batch_size=32, n_parse_threads=5, shuffle_buffer_size=10000):\n",
    "    '''\n",
    "    filenames: 文件名列表\n",
    "    n_readers: 并行程度\n",
    "    batch_size: 批大小\n",
    "    n_parse_threads: 解析的并行程度\n",
    "    shuffle_buffer_size: \n",
    "    '''\n",
    "    filenames_dataset = tf.data.Dataset.list_files(filenames)\n",
    "    filenames_dataset = filenames_dataset.repeat()\n",
    "    dataset = filenames_dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length=n_readers)\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 0.63034356,  1.8741661 , -0.06713215, -0.12543367, -0.19737554,\n",
      "        -0.02272263, -0.69240725,  0.72652334],\n",
      "       [-0.097193  , -1.2497431 ,  0.36232963,  0.02690608,  1.0338118 ,\n",
      "         0.04588159,  1.3418335 , -1.635387  ],\n",
      "       [ 0.09734604,  0.75276285, -0.20218964, -0.19547   , -0.40605137,\n",
      "         0.00678553, -0.81371516,  0.6566148 ]], dtype=float32)>\n",
      "y:\n",
      "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2.419, 1.832, 1.119], dtype=float32)>\n",
      "x:\n",
      "<tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 0.81150836, -0.04823952,  0.5187339 , -0.0293864 , -0.03406402,\n",
      "        -0.05081595, -0.7157357 ,  0.91627514],\n",
      "       [ 0.8015443 ,  0.27216142, -0.11624393, -0.20231152, -0.5430516 ,\n",
      "        -0.02103962, -0.5897621 , -0.08241846],\n",
      "       [ 1.6312258 ,  0.35226166,  0.04080576, -0.14088951, -0.4632104 ,\n",
      "        -0.06751624, -0.82771224,  0.59669316]], dtype=float32)>\n",
      "y:\n",
      "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2.147, 3.226, 3.376], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_read_dataset(train_filenames, batch_size=3)\n",
    "for x_batch, y_batch in train_set.take(2):\n",
    "    print('x:')\n",
    "    pprint.pprint(x_batch)\n",
    "    print('y:')\n",
    "    pprint.pprint(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_read_dataset(train_filenames, batch_size=32)\n",
    "valid_set = csv_read_dataset(valid_filenames, batch_size=32)\n",
    "test_set = csv_read_dataset(test_filenames, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
