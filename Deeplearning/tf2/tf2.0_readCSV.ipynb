{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)\n",
      "tensorflow 2.0.0-beta1\n",
      "numpy 1.18.1\n",
      "pandas 0.24.1\n",
      "sklearn 0.21.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kdd/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in tf, np, pd, sklearn:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从numpy数组构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# repeat 将数据重复几次，相当于epoch的作用\n",
    "# batch 每批次获取的样本数量\n",
    "dataset1 = dataset.repeat(4).batch(5)\n",
    "for item in dataset1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.repeat(3).batch(7):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# interleave 将dataset中元素进行处理，相当于map的功能\n",
    "# cycle_length 多少个Tensor一个循环\n",
    "# block_lenght 一次获取v的多少个元素\n",
    "dataset2 = dataset1.interleave(\n",
    "    lambda v: tf.data.Dataset.from_tensor_slices(v), cycle_length=4, block_length=3)\n",
    "for item in dataset2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从元组/字典构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=363, shape=(3,), dtype=int64, numpy=array([1, 2, 3])>, <tf.Tensor: id=364, shape=(), dtype=string, numpy=b'Beijing'>)\n",
      "(<tf.Tensor: id=367, shape=(3,), dtype=int64, numpy=array([2, 3, 4])>, <tf.Tensor: id=368, shape=(), dtype=string, numpy=b'Shanghai'>)\n",
      "(<tf.Tensor: id=371, shape=(3,), dtype=int64, numpy=array([3, 4, 5])>, <tf.Tensor: id=372, shape=(), dtype=string, numpy=b'Chongqing'>)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3], [2,3,4], [3,4,5]])\n",
    "y = np.array(['Beijing', 'Shanghai', 'Chongqing'])\n",
    "dataset3 = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "for item in dataset3:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] Beijing\n",
      "[2 3 4] Shanghai\n",
      "[3 4 5] Chongqing\n"
     ]
    }
   ],
   "source": [
    "for item_x, item_y in dataset3:\n",
    "    print(item_x.numpy(), item_y.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3], shape=(3,), dtype=int64) tf.Tensor(b'Beijing', shape=(), dtype=string)\n",
      "tf.Tensor([2 3 4], shape=(3,), dtype=int64) tf.Tensor(b'Shanghai', shape=(), dtype=string)\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int64) tf.Tensor(b'Chongqing', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset3:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Beijing' [1 2 3]\n",
      "b'Shanghai' [2 3 4]\n",
      "b'Chongqing' [3 4 5]\n"
     ]
    }
   ],
   "source": [
    "for item in tf.data.Dataset.from_tensor_slices({'coordinate': x, 'city': y}):\n",
    "    print(item['city'].numpy(), item['coordinate'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/kdd/scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "# 获取california_housing数据集\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "house = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15480, 8) (15480,)\n",
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "house.data, house.target,random_state=7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=11)\n",
    "print(x_train_all.shape, y_train_all.shape)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./data/csv/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# 将feature和label拼接起来   \n",
    "train_data = np.c_[x_train_scaled, y_train]\n",
    "valid_data = np.c_[x_valid_scaled, y_valid]\n",
    "test_data = np.c_[x_test_scaled, y_test]\n",
    "# 获取字段名称\n",
    "house_cols = house.feature_names + ['MHV']\n",
    "cols_str = ','.join(house_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个保存csv文件的方法\n",
    "def save_to_csv(output_dir, data, name_prefix, header=None, n_parts=10):\n",
    "    '''\n",
    "    output_dir: 保存路径\n",
    "    data: 要保存的数据\n",
    "    name_prefix: 保存的文件名前缀\n",
    "    header: 表头\n",
    "    n_parts: 保存几部分\n",
    "    '''\n",
    "    path_format = os.path.join(output_dir, '{}_{:2d}.csv')\n",
    "    filenames = []\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(len(data)), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv, 'wt', encoding='utf-8') as f:\n",
    "            if header is not None:\n",
    "                f.write(header+'\\n')\n",
    "            for row_index in row_indices:\n",
    "                f.write(','.join([repr(col) for col in data[row_index]]))\n",
    "                f.write('\\n')\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = save_to_csv(output_dir, train_data, 'train', cols_str, n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, 'valid', cols_str, n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir, test_data, 'test', cols_str, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_filenames:\n",
      "['./data/csv/train_ 0.csv',\n",
      " './data/csv/train_ 1.csv',\n",
      " './data/csv/train_ 2.csv',\n",
      " './data/csv/train_ 3.csv',\n",
      " './data/csv/train_ 4.csv',\n",
      " './data/csv/train_ 5.csv',\n",
      " './data/csv/train_ 6.csv',\n",
      " './data/csv/train_ 7.csv',\n",
      " './data/csv/train_ 8.csv',\n",
      " './data/csv/train_ 9.csv',\n",
      " './data/csv/train_10.csv',\n",
      " './data/csv/train_11.csv',\n",
      " './data/csv/train_12.csv',\n",
      " './data/csv/train_13.csv',\n",
      " './data/csv/train_14.csv',\n",
      " './data/csv/train_15.csv',\n",
      " './data/csv/train_16.csv',\n",
      " './data/csv/train_17.csv',\n",
      " './data/csv/train_18.csv',\n",
      " './data/csv/train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print('train_filenames:')\n",
    "pprint.pprint(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/csv/train_ 0.csv', './data/csv/train_ 1.csv', './data/csv/train_ 2.csv', './data/csv/train_ 3.csv', './data/csv/train_ 4.csv', './data/csv/train_ 5.csv', './data/csv/train_ 6.csv', './data/csv/train_ 7.csv', './data/csv/train_ 8.csv', './data/csv/train_ 9.csv', './data/csv/train_10.csv', './data/csv/train_11.csv', './data/csv/train_12.csv', './data/csv/train_13.csv', './data/csv/train_14.csv', './data/csv/train_15.csv', './data/csv/train_16.csv', './data/csv/train_17.csv', './data/csv/train_18.csv', './data/csv/train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "print(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (), types: tf.string>\n",
      "tf.Tensor(b'./data/csv/train_ 0.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 6.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 9.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 3.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 8.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 2.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 1.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 7.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 5.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'./data/csv/train_ 4.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 构建文件名数据集\n",
    "filenames_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "print(filenames_dataset)\n",
    "for filename in filenames_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801544314532886,0.27216142415910205,-0.11624392696666119,-0.2023115137272354,-0.5430515742518128,-0.021039615516440048,-0.5897620622908205,-0.08241845654707416,3.226\n",
      "-0.32652634129448693,0.43236189741438374,-0.09345459539684739,-0.08402991822890092,0.8460035745154013,-0.0266316482653991,-0.5617679242614233,0.1422875991184281,2.431\n",
      "-0.6672227549433569,-0.04823952235146133,0.34529405473316743,0.5382668657200925,1.8521839533415545,-0.0611253832474835,-0.8417093045554153,1.520484740533546,1.59\n",
      "-1.0775077698160966,-0.44874070548966555,-0.5680568205591913,-0.14269262164909954,-0.09666677138213985,0.12326468238687088,-0.3144863716683942,-0.4818958888413162,0.978\n",
      "0.15782311132800697,0.43236189741438374,0.3379948076652917,-0.015880306122244434,-0.3733890577139493,-0.05305245634489608,0.8006134598360177,-1.2359095422966828,3.169\n"
     ]
    }
   ],
   "source": [
    "n_readers = 5\n",
    "dataset = filenames_dataset.interleave(lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "                                      cycle_length=n_readers)\n",
    "for line in dataset.take(5): # 取前5行\n",
    "    print(line.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor(b'4', shape=(), dtype=string)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 解析csv中的内容， 举个栗子\n",
    "sample_str = '1,2,3,4,5'\n",
    "records = [\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    0,\n",
    "    np.nan,\n",
    "    'hell',\n",
    "    tf.constant([])\n",
    "]\n",
    "parsed_fields = tf.io.decode_csv(sample_str, records)\n",
    "for item in parsed_fields:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Field 4 is required but missing in record 0! [Op:DecodeCSV]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-18d909f1d3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',,,,'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv_v2\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[1;32m   2019\u001b[0m       \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m       \u001b[0mselect_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselect_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m   )\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfield_delim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0muse_quote_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_quote_delim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             select_cols=select_cols, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m     75\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv_eager_fallback\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, select_cols, name, ctx)\u001b[0m\n\u001b[1;32m    149\u001b[0m   _result = _execute.execute(b\"DecodeCSV\", len(record_defaults),\n\u001b[1;32m    150\u001b[0m                              \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_inputs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                              name=name)\n\u001b[0m\u001b[1;32m    152\u001b[0m   _execute.record_gradient(\n\u001b[1;32m    153\u001b[0m       \"DecodeCSV\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m/home/kdd/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kdd/.local/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Field 4 is required but missing in record 0! [Op:DecodeCSV]"
     ]
    }
   ],
   "source": [
    "tf.io.decode_csv(',,,,', records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.io.decode_csv(',,,,', records)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 fields but have 3 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.io.decode_csv('11,2,5', records)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个解析csv每行内容的方法\n",
    "def parse_csv_line(line, n_fields):\n",
    "    records = [tf.constant(np.nan)]*n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, records)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=629, shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.63034356,  1.8741661 , -0.06713215, -0.12543367, -0.19737554,\n",
       "        -0.02272263, -0.69240725,  0.72652334], dtype=float32)>,\n",
       " <tf.Tensor: id=628, shape=(), dtype=float32, numpy=2.419>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_csv_line(b'0.6303435674178064,1.874166156711919,-0.06713214279531016,-0.12543366804152128,-0.19737553788322462,-0.022722631725889016,-0.692407235065288,0.7265233438487496,2.419',n_fields=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个解析csv每行内容的方法\n",
    "def parse_csv_line(line, n_fields=9):\n",
    "    records = [tf.constant(np.nan)]*n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, records)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1])\n",
    "    return x, y\n",
    "\n",
    "# 定义一个多线程读取csv文件并解析的方法\n",
    "def csv_read_dataset(filenames, n_readers=5, batch_size=32, n_parse_threads=5, shuffle_buffer_size=10000):\n",
    "    '''\n",
    "    filenames: 文件名列表\n",
    "    n_readers: 并行程度\n",
    "    batch_size: 批大小\n",
    "    n_parse_threads: 解析的并行程度\n",
    "    shuffle_buffer_size: \n",
    "    '''\n",
    "    filenames_dataset = tf.data.Dataset.list_files(filenames)\n",
    "    filenames_dataset = filenames_dataset.repeat()\n",
    "    dataset = filenames_dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length=n_readers)\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "<tf.Tensor: id=712, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 0.40127665, -0.92934215, -0.0533305 , -0.18659453,  0.65456617,\n",
      "         0.02643447,  0.9312528 , -1.4406418 ],\n",
      "       [ 0.48530516, -0.8492419 , -0.06530126, -0.02337966,  1.4974351 ,\n",
      "        -0.07790658, -0.90236324,  0.78145146],\n",
      "       [ 0.63034356,  1.8741661 , -0.06713215, -0.12543367, -0.19737554,\n",
      "        -0.02272263, -0.69240725,  0.72652334]], dtype=float32)>\n",
      "y:\n",
      "<tf.Tensor: id=713, shape=(3,), dtype=float32, numpy=array([2.512, 2.956, 2.419], dtype=float32)>\n",
      "x:\n",
      "<tf.Tensor: id=716, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 0.04326301, -1.0895426 , -0.38878718, -0.10789865, -0.68186635,\n",
      "        -0.0723871 , -0.8883662 ,  0.8213992 ],\n",
      "       [ 0.81150836, -0.04823952,  0.5187339 , -0.0293864 , -0.03406402,\n",
      "        -0.05081595, -0.7157357 ,  0.91627514],\n",
      "       [-0.8757754 ,  1.8741661 , -0.94874996, -0.09657185, -0.7163432 ,\n",
      "        -0.07790191,  0.98257536, -1.4206679 ]], dtype=float32)>\n",
      "y:\n",
      "<tf.Tensor: id=717, shape=(3,), dtype=float32, numpy=array([1.426, 2.147, 2.75 ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_read_dataset(train_filenames, batch_size=3)\n",
    "for x_batch, y_batch in train_set.take(2):\n",
    "    print('x:')\n",
    "    pprint.pprint(x_batch)\n",
    "    print('y:')\n",
    "    pprint.pprint(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_read_dataset(train_filenames, batch_size=32)\n",
    "valid_set = csv_read_dataset(valid_filenames, batch_size=32)\n",
    "test_set = csv_read_dataset(test_filenames, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
